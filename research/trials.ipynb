{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Github repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data_Science_stuff\\\\Generative_AI_classes\\\\code_analyzer\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'd:\\\\Data_Science_stuff\\\\Generative_AI_classes\\\\code_analyzer\\\\research\\\\test_repo\\\\.git'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "Repo.clone_from(\"https://github.com/srivanoo21/house_price_prediction\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='from fastapi import FastAPI\\nimport uvicorn\\nimport sys\\nimport os\\nfrom fastapi.templating import Jinja2Templates\\nfrom starlette.responses import RedirectResponse\\nfrom fastapi.responses import Response\\nfrom src.house_pricing.pipeline.prediction import PredictionPipeline\\n\\n\\napp = FastAPI()\\n\\n@app.get(\"/\", tags=[\\'authentication\\'])\\nasync def index():\\n    return RedirectResponse(url=\"/docs\")\\n\\n\\n@app.get(\"/train\")\\nasync def training():\\n    try:\\n        os.system(\"python main.py\")\\n        return Response(\"Training successful !!\")\\n    \\n    except Exception as e:\\n        return Response(f\"Error Occurred! {e}\")\\n    \\n\\n@app.post(\"/predict\")\\nasync def predict_route():\\n    try:\\n        obj = PredictionPipeline()\\n        obj.get_transformed_data()\\n        obj.predict_data()\\n        return Response(\"Prediction successful !!\")\\n\\n    except Exception as e:\\n        return Response(f\"Error Occurred! {e}\")\\n\\n\\nif __name__ == \"__main__\":\\n    uvicorn.run(app, host=\"0.0.0.0\", port=5000)', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from src.house_pricing.pipeline.stage_01_data_ingestion import DataIngestionPipeline\\nfrom src.house_pricing.pipeline.stage_02_data_validation import DataValidationPipeline\\nfrom src.house_pricing.pipeline.stage_03_data_transformation import DataTransformationPipeline\\nfrom src.house_pricing.pipeline.stage_04_model_trainer import ModelTrainerPipeline\\nfrom src.house_pricing.pipeline.prediction import PredictionPipeline\\nfrom src.house_pricing.logger import logging\\n\\n\\nSTAGE_NAME = \"Data Ingestion Stage\"\\ntry:\\n    logging.info(f\">>>> stage {STAGE_NAME} started <<<<\")\\n    data_ingestion = DataIngestionPipeline()\\n    data_ingestion.main()\\n    logging.info(f\">>>> stage {STAGE_NAME} completed <<<<\")\\nexcept Exception as e:\\n    logging.exception(e)\\n    raise e\\n\\n\\nSTAGE_NAME = \"Data Validation Stage\"\\ntry:\\n    logging.info(f\">>>> stage {STAGE_NAME} started <<<<\")\\n    data_validation = DataValidationPipeline()\\n    data_validation.main()\\n    logging.info(f\">>>> stage {STAGE_NAME} completed <<<<\")\\nexcept Exception as e:\\n    logging.exception(e)\\n    raise e\\n\\n\\nSTAGE_NAME = \"Data Transformation Stage\"\\ntry:\\n    logging.info(f\">>>> stage {STAGE_NAME} started <<<<\")\\n    data_transformation = DataTransformationPipeline()\\n    data_transformation.main()\\n    logging.info(f\">>>> stage {STAGE_NAME} completed <<<<\")\\nexcept Exception as e:\\n    logging.exception(e)\\n    raise e\\n\\n\\nSTAGE_NAME = \"Model Training\"\\ntry:\\n    logging.info(f\">>>> stage {STAGE_NAME} started <<<<\")\\n    model_train = ModelTrainerPipeline()\\n    model_train.main()\\n    logging.info(f\">>>> stage {STAGE_NAME} completed <<<<\")\\nexcept Exception as e:\\n    logging.exception(e)\\n    raise e\\n\\n\\n# STAGE_NAME = \"Test data prediction\"\\n# try:\\n#     logging.info(f\">>>> stage {STAGE_NAME} started <<<<\")\\n#     pred = PredictionPipeline()\\n#     pred.get_transformed_data()\\n#     pred.predict_data()\\n#     logging.info(f\">>>> stage {STAGE_NAME} completed <<<<\")\\n# except Exception as e:\\n#     logging.exception(e)\\n#     raise e', metadata={'source': 'test_repo\\\\main.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import setuptools\\n\\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as f:\\n    long_description = f.read()\\n\\n__version__ = \"0.0.0\"\\n\\nREPO_NAME = \"House_Price-Prediction\"\\nAUTHOR_USER_NAME = \"srivanoo21\"\\nSRC_REPO = \"house_pricing\"\\nAUTHOR_EMAIL = \"srivanoo21@gmail.com\"\\n\\n\\nsetuptools.setup(\\n\\n    name=SRC_REPO,\\n    version=__version__,\\n    author=AUTHOR_USER_NAME,\\n    author_email=AUTHOR_EMAIL,\\n    description=\"A small python package for house price prediction\",\\n    long_description=long_description,\\n    long_description_content_type=\"text/markdown\",\\n    url=f\"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}\",\\n    project_urls = {\\n        \"Bug Tracker\" : f\"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}/issues\",\\n    },\\n    package_dir={\"\": \"src\"},\\n    packages=setuptools.find_packages(where=\"src\"),\\n\\n)\\n', metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nproject_name = \"house_pricing\"\\n\\n\\n# Below list of files need to be created\\n\\nlist_of_files = [\\n    \".github/workflows/.gitkeep\",\\n    f\"src/{project_name}/__init__.py\",\\n    f\"src/{project_name}/components/__init__.py\",\\n    f\"src/{project_name}/utils/__init__.py\",\\n    f\"src/{project_name}/utils/common.py\",\\n    f\"src/{project_name}/logging/__init__.py\",\\n    f\"src/{project_name}/config/__init__.py\",\\n    f\"src/{project_name}/config/configuration.py\",\\n    f\"src/{project_name}/pipeline/__init__.py\",\\n    f\"src/{project_name}/entity/__init__.py\",\\n    f\"src/{project_name}/constants/__init__.py\",\\n    \"config/config.yaml\",\\n    \"params.yaml\",\\n    \"app.py\",\\n    \"main.py\",\\n    \"Dockerfile\",\\n    \"requirements.txt\",\\n    \"setup.py\",\\n    \"research/trials.ipynb\",\\n    \"test.py\"\\n]\\n\\n\\n# Below is the logic for creation of files and folders\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n    if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory: {filedir} for the file {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) ==0):\\n        with open(filepath, \\'w\\') as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n    else:\\n        logging.info(f\"{filename} already exists\")', metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\test.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nfrom src.house_pricing.logger import logging\\n\\ndef error_message_detail(error,error_detail:sys):\\n    _,_,exc_tb=error_detail.exc_info()\\n    file_name=exc_tb.tb_frame.f_code.co_filename\\n    error_message=\"Error occured in python script name [{0}] line number [{1}] error message[{2}]\".format(\\n     file_name,exc_tb.tb_lineno,str(error))\\n\\n    return error_message\\n\\n    \\nclass CustomException(Exception):\\n    def __init__(self,error_message,error_detail:sys):\\n        super().__init__(error_message)\\n        self.error_message=error_message_detail(error_message,error_detail=error_detail)\\n    \\n    def __str__(self):\\n        return self.error_message\\n    \\n\\n\\n        ', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\exception.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import logging\\nimport os\\nfrom datetime import datetime\\n\\nLOG_FILE=f\"{datetime.now().strftime(\\'%m_%d_%Y_%H_%M_%S\\')}.log\"\\nlogs_path=os.path.join(os.getcwd(),\"logs\",LOG_FILE)\\nos.makedirs(logs_path, exist_ok=True)\\n\\nLOG_FILE_PATH=os.path.join(logs_path,LOG_FILE)\\n\\nlogging.basicConfig(\\n    filename=LOG_FILE_PATH,\\n    format=\"[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s\",\\n    level=logging.INFO,\\n)\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\logger.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nfrom src.house_pricing.exception import CustomException\\nfrom src.house_pricing.logger import logging\\nfrom src.house_pricing.entity import DataIngestionConfig\\nimport pandas as pd\\n\\n\\n\\nclass DataIngestion:\\n    def __init__(self):\\n        self.ingestion_config=DataIngestionConfig()\\n\\n    def initiate_data_ingestion(self):\\n        logging.info(\"Entered the data ingestion method or component\")\\n        try:\\n            train_set = pd.read_csv(\\'data\\\\house_train.csv\\')\\n            logging.info(\\'Read the training dataset as dataframe\\')\\n\\n            test_set = pd.read_csv(\\'data\\\\house_test.csv\\')\\n            logging.info(\\'Read the testing dataset as dataframe\\')\\n\\n            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True)\\n\\n            train_set.to_csv(self.ingestion_config.train_data_path, index=False, header=True)\\n            test_set.to_csv(self.ingestion_config.test_data_path, index=False, header=True)\\n\\n            logging.info(\"Train and test data has been saved\")\\n            logging.info(\"Ingestion of the data is completed\")\\n\\n            return(\\n                self.ingestion_config.train_data_path,\\n                self.ingestion_config.test_data_path\\n            )\\n        except Exception as e:\\n            raise CustomException(e, sys)\\n        \\nif __name__==\"__main__\":\\n    obj = DataIngestion()\\n    train_data, test_data = obj.initiate_data_ingestion()\\n\\n    #data_transformation=DataTransformation()\\n    #train_arr,test_arr,_=data_transformation.initiate_data_transformation(train_data,test_data)\\n\\n    #modeltrainer=ModelTrainer()\\n    #print(modeltrainer.initiate_model_trainer(train_arr,test_arr))\\n\\n\\n\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nimport os\\nimport numpy as np \\nimport pandas as pd\\nfrom scipy.stats import skew\\nfrom dataclasses import dataclass\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\\nfrom src.house_pricing.exception import CustomException\\nfrom src.house_pricing.logger import logging\\nfrom src.house_pricing.entity import DataTransformationConfig\\nfrom src.house_pricing.utils import save_object\\n\\n\\nclass DataTransformation:\\n    def __init__(self):\\n        self.data_transformation_config = DataTransformationConfig()\\n\\n\\n    def cat_con_df(self, test_data):\\n        \\'\\'\\'\\n        Separating the categorical and continuous columns from the testing dataset\\'\\'\\'\\n        \\n        cat = []\\n        con = []\\n\\n        for i in test_data.columns:\\n            if (test_data[i].dtypes==\\'object\\'):\\n                cat.append(i)\\n            else:\\n                con.append(i)\\n        con.remove(\\'Id\\')\\n        \\n        return cat, con\\n\\n\\n    def fill_top_missing_values(self, test_data):\\n        \\'\\'\\'\\n        Check and fill the top 5 missing columns with \\'0\\' in the testing dataset\\'\\'\\'\\n\\n        miss1 = (test_data.isna().sum()/test_data.shape[0])*100\\n        miss1 = pd.DataFrame(miss1, columns=[\\'count\\'])\\n        miss1 = miss1.sort_values(by=\\'count\\', ascending=False)\\n\\n        logging.info(f\"Top 10 missing features from testing dataset are: {miss1[:10]}\")\\n\\n        test_miss = (miss1[:6].index).values\\n\\n        for i in test_miss:\\n            test_data[i].fillna(\"0\", inplace=True)\\n        \\n        logging.info(f\"Missing features from testing dataset are now replaced with \\'0\\'\")\\n            \\n        return test_data\\n\\n\\n    def fill_missing_values(self, test_data):\\n        \\'\\'\\'\\n        Removing the rest of the missing columns from the testing dataset\\'\\'\\'\\n\\n        cat, con = self.cat_con_df(test_data)\\n        test_data = self.fill_top_missing_values(test_data)\\n\\n        si1 = SimpleImputer(strategy=\\'mean\\')\\n        si2 = SimpleImputer(strategy=\\'most_frequent\\')\\n\\n        A = pd.DataFrame(si1.fit_transform(test_data[con]), columns=con)\\n        B = pd.DataFrame(si2.fit_transform(test_data[cat]), columns=cat)\\n\\n        test_new = A.join(B)\\n        logging.info(f\"Missing features from testing dataset are now handled\")\\n        \\n        return test_new, cat, con\\n\\n    \\n    def check_skew(self, test_data):\\n        \\'\\'\\'\\n        Check the skewness of the testing data\\'\\'\\'\\n        \\n        test_data, cat, con = self.fill_missing_values(test_data)\\n\\n        skewed = test_data[con].apply(lambda x: skew(x))\\n        skewed = skewed[skewed > 0.75]\\n        skewed = skewed.index\\n\\n        test_data[skewed] = np.log1p(test_data[skewed])\\n        logging.info(f\"skewness has now been removed from the testing data\")\\n        \\n        return test_data, cat, con\\n    \\n\\n    def scaling(self, test_data):\\n        \\'\\'\\'\\n        perform scaling of the test data\\'\\'\\'\\n        \\n        ss = StandardScaler()\\n        test_data, cat, con = self.check_skew(test_data)\\n        ## SS object need to be in artifacts\\n        test_data[con] = ss.fit_transform(test_data[con])\\n        logging.info(f\"Feature scaling is now performed\")\\n        \\n        return test_data, cat, con\\n    \\n\\n    def handle_outliers(self, test_data):\\n        \\'\\'\\'\\n        Removing the outliers from the below columns as they have more number of outliers\\'\\'\\'\\n        test_data, cat, con = self.scaling(test_data)\\n        val1 = [\\'BsmtUnfSF\\', \\'TotalBsmtSF\\', \\'KitchenAbvGr\\', \\'ScreenPorch\\']\\n\\n        for i in val1:\\n            Q1 = test_data[i].quantile(0.05)\\n            Q3 = test_data[i].quantile(0.95)\\n            IQR = Q3 - Q1\\n            test_data = test_data[(test_data[i] >= Q1 - 1.5*IQR) & (test_data[i] <= Q3 + 1.5*IQR)]\\n\\n        logging.info(f\"Outliers are now handled in the testing data\")\\n            \\n        return test_data, cat, con\\n    \\n\\n    def encode_test_data(self, test_data):\\n        \\'\\'\\'\\n        perform one hot encoding so as to handle unseen values\\'\\'\\'\\n        \\n        test_data, cat, con = self.handle_outliers(test_data)\\n        le = LabelEncoder()\\n        for i in cat:\\n            test_data[i] = le.fit_transform(test_data[i])\\n            \\n        logging.info(f\"Missing features from testing dataset are now replaced with \\'0\\'\")\\n        return test_data\\n    \\n\\n    def initiate_data_transformation(self):\\n        \\'\\'\\'\\n        Start the data transformation process\\'\\'\\'\\n\\n        try:\\n            test_data = pd.read_csv(self.data_transformation_config.test_data_path)\\n            logging.info(\"Read testing data is completed\")\\n\\n            test_data = self.encode_test_data(test_data)\\n            logging.info(f\"Data pre-processing is completed for testing data\")\\n            \\n            os.makedirs(os.path.dirname(self.data_transformation_config.preprocessed_test_data_path), \\n                        exist_ok=True)\\n            logging.info(\"The directory for the data transformation is now created\")\\n\\n            logging.info(f\"Saving preprocessed testing data.\")\\n\\n            save_object(\\n                file_path = self.data_transformation_config.preprocessed_test_data_path,\\n                obj = test_data\\n            )\\n            logging.info(f\"Saved preprocessed testing data.\")\\n\\n            return self.data_transformation_config.preprocessed_test_data_path\\n            \\n        except Exception as e:\\n            raise CustomException(e,sys)\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\components\\\\data_transformation_prediction.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nimport os\\nimport numpy as np \\nimport pandas as pd\\nfrom scipy.stats import skew\\nfrom dataclasses import dataclass\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\\nfrom src.house_pricing.exception import CustomException\\nfrom src.house_pricing.logger import logging\\nfrom src.house_pricing.entity import DataTransformationConfig\\n\\n\\n\\nclass DataTransformation:\\n    def __init__(self):\\n        self.data_transformation_config = DataTransformationConfig()\\n\\n\\n    def cat_con_df(self, train_data):\\n        \\'\\'\\'\\n        Separating the categorical and continuous columns from the training dataset\\'\\'\\'\\n        \\n        cat = []\\n        con = []\\n\\n        for i in train_data.columns:\\n            if (train_data[i].dtypes==\\'object\\'):\\n                cat.append(i)\\n            else:\\n                con.append(i)\\n        con.remove(\\'Id\\')\\n        \\n        return cat, con\\n\\n\\n    def fill_top_missing_values(self, train_data):\\n        \\'\\'\\'\\n        Check and fill the top 5 missing columns with \\'0\\' in the training dataset\\'\\'\\'\\n\\n        miss1 = (train_data.isna().sum()/train_data.shape[0])*100\\n        miss1 = pd.DataFrame(miss1, columns=[\\'count\\'])\\n        miss1 = miss1.sort_values(by=\\'count\\', ascending=False)\\n\\n        logging.info(f\"Top 10 missing features from training dataset are: {miss1[:10]}\")\\n\\n        train_miss = (miss1[:6].index).values\\n\\n        for i in train_miss:\\n            train_data[i].fillna(\"0\", inplace=True)\\n        \\n        logging.info(f\"Missing features from training dataset are now replaced with \\'0\\'\")\\n            \\n        return train_data\\n\\n\\n    def fill_missing_values(self, train_data):\\n        \\'\\'\\'\\n        Removing the rest of the missing columns from the training dataset\\'\\'\\'\\n\\n        cat, con = self.cat_con_df(train_data)\\n        train_data = self.fill_top_missing_values(train_data)\\n\\n        si1 = SimpleImputer(strategy=\\'mean\\')\\n        si2 = SimpleImputer(strategy=\\'most_frequent\\')\\n\\n        A = pd.DataFrame(si1.fit_transform(train_data[con]), columns=con)\\n        B = pd.DataFrame(si2.fit_transform(train_data[cat]), columns=cat)\\n\\n        train_new = A.join(B)\\n        logging.info(f\"Missing features from training dataset are now handled\")\\n        \\n        return train_new, cat, con\\n\\n    \\n    def check_skew(self, train_data):\\n        \\'\\'\\'\\n        Check the skewness of the training data\\'\\'\\'\\n        \\n        train_data, cat, con = self.fill_missing_values(train_data)\\n\\n        con.remove(\\'SalePrice\\')\\n        skewed = train_data[con].apply(lambda x: skew(x))\\n        skewed = skewed[skewed > 0.75]\\n        skewed = skewed.index\\n\\n        train_data[skewed] = np.log1p(train_data[skewed])\\n        con.append(\\'SalePrice\\')\\n        logging.info(f\"skewness has now been removed from the training data\")\\n        \\n        return train_data, cat, con\\n\\n\\n    def scaling(self, train_data):\\n        \\'\\'\\'\\n        perform scaling of the train data\\'\\'\\'\\n        \\n        ss = StandardScaler()\\n        train_data, cat, con = self.check_skew(train_data)\\n\\n        con.remove(\\'SalePrice\\')\\n        train_data[con] = ss.fit_transform(train_data[con])\\n        con.append(\\'SalePrice\\')\\n        logging.info(f\"Feature scaling is now performed\")\\n        \\n        return train_data, cat, con\\n    \\n\\n    def handle_outliers(self, train_data):\\n        \\'\\'\\'\\n        Removing the outliers from the below columns as they have more number of outliers\\'\\'\\'\\n        train_data, cat, con = self.scaling(train_data)\\n        val1 = [\\'BsmtUnfSF\\', \\'TotalBsmtSF\\', \\'KitchenAbvGr\\', \\'ScreenPorch\\']\\n\\n        for i in val1:\\n            Q1 = train_data[i].quantile(0.05)\\n            Q3 = train_data[i].quantile(0.95)\\n            IQR = Q3 - Q1\\n            train_data = train_data[(train_data[i] >= Q1 - 1.5*IQR) & (train_data[i] <= Q3 + 1.5*IQR)]\\n\\n        logging.info(f\"Outliers are now handled in the training data\")\\n            \\n        return train_data, cat, con\\n    \\n\\n    def encode_train_data(self, train_data):\\n        \\'\\'\\'\\n        perform one hot encoding so as to handle unseen values\\'\\'\\'\\n        train_data, cat, con = self.handle_outliers(train_data)\\n        le = LabelEncoder()\\n        for i in cat:\\n            train_data[i] = le.fit_transform(train_data[i])\\n            \\n        logging.info(f\"Missing features from training dataset are now replaced with \\'0\\'\")\\n        return train_data\\n    \\n\\n    def initiate_data_transformation(self):\\n        \\'\\'\\'\\n        Start the data transformation process\\'\\'\\'\\n\\n        try:\\n            train_data = pd.read_csv(self.data_transformation_config.train_data_path)\\n            logging.info(\"Read training data is completed\")\\n\\n            train_data = self.encode_train_data(train_data)\\n            logging.info(f\"Data pre-processing is completed for training data\")\\n            \\n            os.makedirs(os.path.dirname(self.data_transformation_config.preprocessed_train_data_path), \\n                        exist_ok=True)\\n            logging.info(\"The directory for the data transformation is now created\")\\n\\n            logging.info(f\"Saving preprocessed training data.\")\\n            train_data.to_csv(self.data_transformation_config.preprocessed_train_data_path, \\n                              index=False, header=True)\\n            \\n            logging.info(f\"Saved preprocessed training data.\")\\n\\n            return self.data_transformation_config.preprocessed_train_data_path\\n            \\n        except Exception as e:\\n            raise CustomException(e,sys)\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\components\\\\data_transformation_train.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom src.house_pricing.logger import logging\\nfrom src.house_pricing.entity import DataValidationConfig\\n\\n\\nclass DataValidation:\\n    \\n    def __init__(self, config: DataValidationConfig):\\n        self.config = config\\n\\n    def validate_all_files_exist(self) -> bool:\\n        try:\\n            validation_status = None\\n            all_files = os.listdir(os.path.join(\"artifacts\", \"data_ingestion\"))\\n            self.config.ALL_REQUIRED_FILES = [\"house_training_data.csv\", \"house_testing_data.csv\"]\\n\\n            # create the status file\\n            os.makedirs(self.config.root_dir, exist_ok=True)\\n            logging.info(\"Directory created for data validation\")\\n\\n            status_file = os.path.join(self.config.root_dir, self.config.STATUS_FILE)\\n\\n            for file in all_files:\\n                if (file not in self.config.ALL_REQUIRED_FILES):\\n                    validation_status = False\\n                    with open(status_file, \"w\") as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n                    return validation_status\\n                else:\\n                    validation_status = True\\n                    with open(status_file, \"w\") as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n\\n            logging.info(\"Validation status has been updated\")\\n            \\n        except Exception as e:\\n            raise e\\n        \\n        return validation_status\\n        ', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\components\\\\data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nimport os\\nimport numpy as np \\nimport pandas as pd\\nfrom scipy.stats import skew\\nfrom dataclasses import dataclass\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\\nfrom src.house_pricing.exception import CustomException\\nfrom src.house_pricing.logger import logging\\nfrom src.house_pricing.entity import ModelTrainerConfig\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import r2_score\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\\nfrom xgboost import XGBRegressor\\nfrom src.house_pricing.utils import save_object\\n\\n\\nclass ModelTrainer:\\n    def __init__(self):\\n        self.model_trainer_config = ModelTrainerConfig()\\n\\n\\n    def train_val_split(self, train_data, target_column):\\n        \\'\\'\\'\\n        split the input data into training and validation set\\'\\'\\'\\n\\n        X = train_data.drop(labels=[target_column], axis=1)\\n        Y = train_data[target_column]\\n        logging.info(\"Data spitting for input and target columns are now prepared\")\\n\\n        xtrain, xval, ytrain, yval = train_test_split(X, Y, test_size=0.2, random_state=40)\\n        logging.info(f\"The shape of training data is {xtrain.shape}\")\\n        logging.info(f\"The shape of validation data is {xval.shape}\")\\n        \\n        return xtrain, xval, ytrain, yval\\n\\n\\n\\n    def initiate_model_trainer(self):\\n        \\'\\'\\'\\n        initiate the model training for the training set\\'\\'\\'\\n\\n        try:\\n            train_data_path = self.model_trainer_config.preprocessed_train_data_path\\n            train_data = pd.read_csv(train_data_path)\\n            target_column = self.model_trainer_config.target_column\\n            logging.info(\"Pre-processed training dataset and target column is now loaded\")\\n\\n            xtrain, xval, ytrain, yval = self.train_val_split(train_data, target_column)\\n            logging.info(\"Splitting of training and validation data is now completed\")\\n\\n            models = {\\n                \"Linear Regression\": LinearRegression(),\\n                \"Decision Tree\": DecisionTreeRegressor(),\\n                \"Random Forest\": RandomForestRegressor(),\\n                \"AdaBoost Regressor\": AdaBoostRegressor()\\n                # \"Gradient Boosting\": GradientBoostingRegressor(),\\n                # \"XGBRegressor\": XGBRegressor()\\n            }\\n\\n            params={\\n                \"Linear Regression\":{},\\n                \"Decision Tree\": {\\n                    \\'criterion\\':[\\'squared_error\\', \\'friedman_mse\\', \\'absolute_error\\', \\'poisson\\'],\\n                    # \\'splitter\\':[\\'best\\',\\'random\\'],\\n                    # \\'max_features\\':[\\'sqrt\\',\\'log2\\'],\\n                },\\n                \"Random Forest\":{\\n                    # \\'criterion\\':[\\'squared_error\\', \\'friedman_mse\\', \\'absolute_error\\', \\'poisson\\'],\\n                 \\n                    # \\'max_features\\':[\\'sqrt\\',\\'log2\\',None],\\n                    \\'n_estimators\\': [8,16,32,64,128,256]\\n                },\\n                # \"Gradient Boosting\":{\\n                #     # \\'loss\\':[\\'squared_error\\', \\'huber\\', \\'absolute_error\\', \\'quantile\\'],\\n                #     \\'learning_rate\\':[.1,.01,.05,.001],\\n                #     \\'subsample\\':[0.6,0.7,0.75,0.8,0.85,0.9],\\n                #     # \\'criterion\\':[\\'squared_error\\', \\'friedman_mse\\'],\\n                #     # \\'max_features\\':[\\'auto\\',\\'sqrt\\',\\'log2\\'],\\n                #     \\'n_estimators\\': [8,16,32,64,128,256]\\n                # },\\n                \\n                # \"XGBRegressor\":{\\n                #     \\'learning_rate\\':[.1,.01,.05,.001],\\n                #     \\'n_estimators\\': [8,16,32,64,128,256]\\n                # },\\n                # \"CatBoosting Regressor\":{\\n                #     \\'depth\\': [6,8,10],\\n                #     \\'learning_rate\\': [0.01, 0.05, 0.1],\\n                #     \\'iterations\\': [30, 50, 100]\\n                # },\\n                \"AdaBoost Regressor\":{\\n                    \\'learning_rate\\':[.1,.01,0.5,.001],\\n                    # \\'loss\\':[\\'linear\\',\\'square\\',\\'exponential\\'],\\n                    \\'n_estimators\\': [8,16,32,64,128,256]\\n                }   \\n            }\\n\\n            train_report, val_report, cross_val_report, adj_r2_report = self.evaluate_models(\\n                train_data, target_column, xtrain, ytrain, xval, yval, models, params)\\n            \\n            logging.info(f\"The training report is {train_report}\")\\n            logging.info(f\"The validation report is {val_report}\")\\n            logging.info(f\"The cross validation report is {cross_val_report}\")\\n            logging.info(f\"The adj_r2_report report is {adj_r2_report}\")\\n            \\n            ## To get best model score from dict\\n            best_train_score = max(sorted(train_report.values()))\\n            best_val_score = max(sorted(val_report.values()))\\n            best_cross_val_score = max(sorted(cross_val_report.values()))\\n            best_adj_r2_score = max(sorted(adj_r2_report.values()))\\n\\n            ## To get best model name from dict on different areas\\n            best_model_name_train = list(train_report.keys())[\\n                list(train_report.values()).index(best_train_score)]\\n            best_model_train = models[best_model_name_train]\\n\\n            best_model_name_val = list(val_report.keys())[\\n                list(val_report.values()).index(best_val_score)]\\n            best_model_val = models[best_model_name_val]\\n\\n            best_model_name_cross_val = list(cross_val_report.keys())[\\n                list(cross_val_report.values()).index(best_cross_val_score)]\\n            best_model_cross_val = models[best_model_name_cross_val]\\n\\n            best_model_name_adj_r2 = list(adj_r2_report.keys())[\\n                list(adj_r2_report.values()).index(best_adj_r2_score)]\\n            best_model_adj_r2 = models[best_model_name_adj_r2]\\n\\n            # Displaying which model provides best score in which area\\n            logging.info(f\"Best model for training score is: {best_model_train}\")\\n            logging.info(f\"Best model for validation score is: {best_model_val}\")\\n            logging.info(f\"Best model for cross validation score is: {best_model_cross_val}\")\\n            logging.info(f\"Best model for adjusted r2 score is: {best_model_adj_r2}\")\\n\\n            # Compare the best model for adjusted r2 score is for the threshold\\n            if best_adj_r2_score < 0.6:\\n                raise CustomException(\"No best model found\")\\n            logging.info(f\"Best found model on both training and validation dataset\")\\n\\n            save_object(\\n                file_path = self.model_trainer_config.trained_model_file_path,\\n                obj = best_model_adj_r2\\n            )\\n            logging.info(f\"Best Model is now saved in the artifacts in {self.model_trainer_config.trained_model_file_path}\")\\n                        \\n        except Exception as e:\\n            raise CustomException(e, sys)\\n        \\n\\n\\n    def evaluate_models(self, train_data, target_column, xtrain, ytrain, xval, yval, models, param):\\n        \\'\\'\\'\\n        evaluate all the models using various metrics\\'\\'\\'\\n\\n        try:\\n            report_train_score = {}\\n            report_val_score = {}\\n            report_cross_val_score = {}\\n            report_adjusted_r2_score = {}\\n\\n            for i in range(len(list(models))):\\n                model = list(models.values())[i]\\n                para = param[list(models.keys())[i]]\\n\\n                gs = GridSearchCV(model, para, cv=3)\\n                gs.fit(xtrain, ytrain)\\n\\n                model.set_params(**gs.best_params_)\\n                model.fit(xtrain, ytrain)\\n                mean_cross_val_score = np.abs(np.mean(cross_val_score(model, xtrain, ytrain, scoring=\\'neg_mean_absolute_error\\', cv=3)))\\n\\n                ytrain_pred = model.predict(xtrain)\\n                yval_pred = model.predict(xval)\\n                \\n                train_model_score = r2_score(ytrain, ytrain_pred)\\n                val_model_score = r2_score(yval, yval_pred)\\n\\n                n = xval.shape[0]\\n                p = xval.shape[1]\\n                adjusted_val_score = 1-(1-val_model_score)*(n-1)/(n-p-1)\\n\\n                report_train_score[list(models.keys())[i]] = train_model_score\\n                report_val_score[list(models.keys())[i]] = val_model_score\\n                report_cross_val_score[list(models.keys())[i]] = mean_cross_val_score\\n                report_adjusted_r2_score[list(models.keys())[i]] = adjusted_val_score\\n\\n            return report_train_score, report_val_score, report_cross_val_score, report_adjusted_r2_score\\n\\n        except Exception as e:\\n            raise CustomException(e, sys)\\n        ', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\components\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom dataclasses import dataclass\\nfrom src.house_pricing.constants import *\\n\\n\\nclass ConfigurationManager:\\n    \\n    def __init__(self, config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH):\\n        \\n        #self.config = read_yaml(config_filepath)\\n        #self.params = read_yaml(params_filepath)\\n\\n        #create_directories([self.config.artifacts_root])\\n        pass', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\config\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pathlib import Path\\n\\nCONFIG_FILE_PATH = Path(\"config/config.yaml\")\\nPARAMS_FILE_PATH = Path(\"params.yaml\")\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\constants\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from dataclasses import dataclass\\nimport os\\n\\n@dataclass\\nclass DataIngestionConfig:\\n    train_data_path: str = os.path.join(\\'artifacts\\', \"data_ingestion\", \"house_training_data.csv\")\\n    test_data_path: str = os.path.join(\\'artifacts\\', \"data_ingestion\", \"house_testing_data.csv\")\\n\\n\\n@dataclass\\nclass DataValidationConfig:\\n    ALL_REQUIRED_FILES: list\\n    root_dir: str = os.path.join(\"artifacts\", \"data_validation\")\\n    STATUS_FILE: str = \"status.txt\"\\n\\n\\n@dataclass\\nclass DataTransformationConfig:\\n    train_data_path = os.path.join(\\'artifacts\\', \"data_ingestion\", \"house_training_data.csv\")\\n    test_data_path = os.path.join(\\'artifacts\\', \"data_ingestion\", \"house_testing_data.csv\")\\n    preprocessed_train_data_path = os.path.join(\\'artifacts\\', \"data_transformation\", \"preprocesed_train_data.csv\")\\n    preprocessed_test_data_path = os.path.join(\\'artifacts\\', \"data_transformation\", \"preprocesed_test_data.csv\")\\n\\n\\n@dataclass\\nclass ModelTrainerConfig:\\n    target_column: str = \"SalePrice\"\\n    preprocessed_train_data_path = os.path.join(\\'artifacts\\', \"data_transformation\", \"preprocesed_train_data.csv\")\\n    trained_model_file_path: str = os.path.join(\"artifacts\", \"model_train\", \"model.pkl\")\\n\\n\\n@dataclass\\nclass PredictionConfig:\\n    predicted_column: str = \"SalePrice\"\\n    preprocessed_test_data_path = os.path.join(\\'artifacts\\', \"data_transformation\", \"preprocesed_test_data.csv\")\\n    trained_model_file_path: str = os.path.join(\"artifacts\", \"model_train\", \"model.pkl\")\\n    predicted_data_path = os.path.join(\"artifacts\", \"data_prediction\", \"predicted_data.csv\")', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\entity\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='#from src.house_pricing.config.configuration import ConfigurationManager\\nfrom src.house_pricing.components.model_trainer import ModelTrainer\\nfrom src.house_pricing.entity import PredictionConfig\\nfrom src.house_pricing.components.data_transformation_prediction import DataTransformation\\nfrom src.house_pricing.logger import logging\\nfrom src.house_pricing.utils import save_object, load_object\\n\\n\\nclass PredictionPipeline:\\n    def __init__(self):\\n        self.config = PredictionConfig()\\n\\n\\n    def get_transformed_data(self):\\n        \\'\\'\\'\\n        get the transformed data for the prediction\\'\\'\\'\\n\\n        try:\\n            #config = ConfigurationManager()\\n            #data_validation_config = config.get_data_validation_config()\\n            data_transformation = DataTransformation()\\n            data_transformation.initiate_data_transformation()\\n\\n        except Exception as e:\\n            raise e\\n        \\n\\n    def predict_data(self):\\n        \\'\\'\\'\\n        perform prediction on test data and return the output along with the predicted values\\'\\'\\'\\n\\n        try:\\n            test_data_path = self.config.preprocessed_test_data_path\\n            test_data = load_object(file_path = test_data_path)\\n            logging.info(\"Test data to be predicted is fetched\")\\n                         \\n            model_path = self.config.trained_model_file_path\\n            model = load_object(file_path = model_path)\\n            logging.info(\"Model is loaded from the artifacts\")\\n\\n            predicted_column = self.config.predicted_column\\n            \\n            predicted_data = model.predict(test_data)\\n            logging.info(\"Prediction is performed on the test data\")\\n\\n            test_data[predicted_column] = predicted_data\\n            logging.info(\"Predicted data is now added in the test dataset\")\\n\\n            save_object(\\n                file_path = self.config.predicted_data_path,\\n                obj = test_data\\n            )\\n            logging.info(\"Test dataset after the prediction is now saved successfully in the artifacts\")\\n\\n        except Exception as e:\\n            raise e\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from src.house_pricing.config.configuration import ConfigurationManager\\nfrom src.house_pricing.components.data_ingestion import DataIngestion\\nfrom src.house_pricing.logger import logging\\n\\n\\nclass DataIngestionPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        try:\\n            #config = ConfigurationManager()\\n            #data_ingestion_config = config.get_data_ingestion_config()\\n            data_ingestion = DataIngestion()\\n            train_data, test_data = data_ingestion.initiate_data_ingestion()\\n\\n        except Exception as e:\\n            raise e', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\pipeline\\\\stage_01_data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='#from src.house_pricing.config.configuration import ConfigurationManager\\nfrom src.house_pricing.components.data_validation import DataValidation\\nfrom src.house_pricing.entity import DataValidationConfig\\nfrom src.house_pricing.logger import logging\\n\\n\\nclass DataValidationPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        try:\\n            pass\\n            #config = ConfigurationManager()\\n            #data_validation_config = config.get_data_validation_config()\\n            data_validation = DataValidation(config=DataValidationConfig)\\n            data_validation.validate_all_files_exist()\\n\\n        except Exception as e:\\n            raise e\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\pipeline\\\\stage_02_data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='#from src.house_pricing.config.configuration import ConfigurationManager\\nfrom src.house_pricing.components.data_transformation_train import DataTransformation\\nfrom src.house_pricing.entity import DataTransformationConfig\\nfrom src.house_pricing.logger import logging\\n\\n\\nclass DataTransformationPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        try:\\n            pass\\n            #config = ConfigurationManager()\\n            #data_validation_config = config.get_data_validation_config()\\n            data_transformation = DataTransformation()\\n            data_transformation.initiate_data_transformation()\\n\\n        except Exception as e:\\n            raise e\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\pipeline\\\\stage_03_data_transformation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='#from src.house_pricing.config.configuration import ConfigurationManager\\nfrom src.house_pricing.components.model_trainer import ModelTrainer\\nfrom src.house_pricing.entity import ModelTrainerConfig\\nfrom src.house_pricing.logger import logging\\n\\n\\nclass ModelTrainerPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        try:\\n            pass\\n            #config = ConfigurationManager()\\n            #data_validation_config = config.get_data_validation_config()\\n            model_train = ModelTrainer()\\n            model_train.initiate_model_trainer()\\n\\n        except Exception as e:\\n            raise e\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\pipeline\\\\stage_04_model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\pipeline\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\utils\\\\common.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nimport numpy as np \\nimport pandas as pd\\nimport pickle\\nfrom sklearn.metrics import r2_score\\nfrom sklearn.model_selection import GridSearchCV\\nfrom src.house_pricing.exception import CustomException\\n\\n    \\ndef save_object(file_path, obj):\\n    try:\\n        dir_path = os.path.dirname(file_path)\\n\\n        os.makedirs(dir_path, exist_ok=True)\\n\\n        with open(file_path, \"wb\") as file_obj:\\n            pickle.dump(obj, file_obj, pickle.DEFAULT_PROTOCOL)\\n\\n    except Exception as e:\\n        raise CustomException(e, sys)\\n\\n    \\n    \\ndef load_object(file_path):\\n    try:\\n        with open(file_path, \"rb\") as file_obj:\\n            return pickle.load(file_obj)\\n\\n    except Exception as e:\\n        raise CustomException(e, sys)\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\utils\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 2000,\n",
    "                                                             chunk_overlap = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge base (vector DB)\n",
    "\n",
    "Here we are using the Chroma library to create a vector database (vectordb) from a list of text documents (texts) with associated embeddings (embeddings).\n",
    "\n",
    "The 'persist_directory' is a directory where the vector database will be stored as arguments.\n",
    "After creating the vector database, 'persist()' is called to save the vector database to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected EmbeddingFunction.__call__ to have the following signature: odict_keys(['self', 'input']), got odict_keys(['self', 'args', 'kwargs'])\nPlease see https://docs.trychroma.com/embeddings for details of the EmbeddingFunction interface.\nPlease note the recent change to the EmbeddingFunction interface: https://docs.trychroma.com/migration#migration-to-0416---november-7-2023 \n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m vectordb\u001b[38;5;241m.\u001b[39mpersist()\n",
      "File \u001b[1;32md:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:592\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    591\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 592\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:547\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[Chroma],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    527\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Chroma:\n\u001b[0;32m    528\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \n\u001b[0;32m    530\u001b[0m \u001b[38;5;124;03m    If a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m        Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 547\u001b[0m     chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[1;32md:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\langchain\\vectorstores\\chroma.py:115\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[1;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persist_directory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    111\u001b[0m         _client_settings\u001b[38;5;241m.\u001b[39mpersist_directory \u001b[38;5;129;01mor\u001b[39;00m persist_directory\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;241m=\u001b[39m embedding_function\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverride_relevance_score_fn \u001b[38;5;241m=\u001b[39m relevance_score_fn\n",
      "File \u001b[1;32md:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\chromadb\\api\\client.py:237\u001b[0m, in \u001b[0;36mClient.get_or_create_collection\u001b[1;34m(self, name, metadata, embedding_function, data_loader)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_or_create_collection\u001b[39m(\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    235\u001b[0m     data_loader: Optional[DataLoader[Loadable]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[1;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\chromadb\\api\\segment.py:217\u001b[0m, in \u001b[0;36mSegmentAPI.get_or_create_collection\u001b[1;34m(self, name, metadata, embedding_function, data_loader, tenant, database)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmentAPI.get_or_create_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m, OpenTelemetryGranularity\u001b[38;5;241m.\u001b[39mOPERATION\n\u001b[0;32m    204\u001b[0m )\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[0;32m    216\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\chromadb\\telemetry\\opentelemetry\\__init__.py:127\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\chromadb\\api\\segment.py:191\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[1;34m(self, name, metadata, embedding_function, data_loader, get_or_create, tenant, database)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_product_telemetry_client\u001b[38;5;241m.\u001b[39mcapture(\n\u001b[0;32m    184\u001b[0m     ClientCreateCollectionEvent(\n\u001b[0;32m    185\u001b[0m         collection_uuid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m),\n\u001b[0;32m    186\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    188\u001b[0m )\n\u001b[0;32m    189\u001b[0m add_attributes_to_current_span({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_uuid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m)})\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoll\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoll\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\chromadb\\api\\models\\Collection.py:87\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[1;34m(self, client, name, id, embedding_function, data_loader, tenant, database, metadata)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Check to make sure the embedding function has the right signature, as defined by the EmbeddingFunction protocol\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m     \u001b[43mvalidate_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;241m=\u001b[39m embedding_function\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader \u001b[38;5;241m=\u001b[39m data_loader\n",
      "File \u001b[1;32md:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\chromadb\\api\\types.py:211\u001b[0m, in \u001b[0;36mvalidate_embedding_function\u001b[1;34m(embedding_function)\u001b[0m\n\u001b[0;32m    208\u001b[0m protocol_signature \u001b[38;5;241m=\u001b[39m signature(EmbeddingFunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_signature \u001b[38;5;241m==\u001b[39m protocol_signature:\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected EmbeddingFunction.__call__ to have the following signature: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprotocol_signature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_signature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see https://docs.trychroma.com/embeddings for details of the EmbeddingFunction interface.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease note the recent change to the EmbeddingFunction interface: https://docs.trychroma.com/migration#migration-to-0416---november-7-2023 \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected EmbeddingFunction.__call__ to have the following signature: odict_keys(['self', 'input']), got odict_keys(['self', 'args', 'kwargs'])\nPlease see https://docs.trychroma.com/embeddings for details of the EmbeddingFunction interface.\nPlease note the recent change to the EmbeddingFunction interface: https://docs.trychroma.com/migration#migration-to-0416---november-7-2023 \n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./data')\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3}), memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question = \"what is DataIngestion class?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
