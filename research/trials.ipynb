{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Github repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data_Science_stuff\\\\Generative_AI_classes\\\\code_analyzer\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'd:\\\\Data_Science_stuff\\\\Generative_AI_classes\\\\code_analyzer\\\\research\\\\test_repo\\\\.git'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "Repo.clone_from(\"https://github.com/srivanoo21/house_price_prediction\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='from fastapi import FastAPI\\nimport uvicorn\\nimport sys\\nimport os\\nfrom fastapi.templating import Jinja2Templates\\nfrom starlette.responses import RedirectResponse\\nfrom fastapi.responses import Response\\nfrom src.house_pricing.pipeline.prediction import PredictionPipeline\\n\\n\\napp = FastAPI()\\n\\n@app.get(\"/\", tags=[\\'authentication\\'])\\nasync def index():\\n    return RedirectResponse(url=\"/docs\")\\n\\n\\n@app.get(\"/train\")\\nasync def training():\\n    try:\\n        os.system(\"python main.py\")\\n        return Response(\"Training successful !!\")\\n    \\n    except Exception as e:\\n        return Response(f\"Error Occurred! {e}\")\\n    \\n\\n@app.post(\"/predict\")\\nasync def predict_route():\\n    try:\\n        obj = PredictionPipeline()\\n        obj.get_transformed_data()\\n        obj.predict_data()\\n        return Response(\"Prediction successful !!\")\\n\\n    except Exception as e:\\n        return Response(f\"Error Occurred! {e}\")\\n\\n\\nif __name__ == \"__main__\":\\n    uvicorn.run(app, host=\"0.0.0.0\", port=5000)', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from src.house_pricing.pipeline.stage_01_data_ingestion import DataIngestionPipeline\\nfrom src.house_pricing.pipeline.stage_02_data_validation import DataValidationPipeline\\nfrom src.house_pricing.pipeline.stage_03_data_transformation import DataTransformationPipeline\\nfrom src.house_pricing.pipeline.stage_04_model_trainer import ModelTrainerPipeline\\nfrom src.house_pricing.pipeline.prediction import PredictionPipeline\\nfrom src.house_pricing.logger import logging\\n\\n\\nSTAGE_NAME = \"Data Ingestion Stage\"\\ntry:\\n    logging.info(f\">>>> stage {STAGE_NAME} started <<<<\")\\n    data_ingestion = DataIngestionPipeline()\\n    data_ingestion.main()\\n    logging.info(f\">>>> stage {STAGE_NAME} completed <<<<\")\\nexcept Exception as e:\\n    logging.exception(e)\\n    raise e\\n\\n\\nSTAGE_NAME = \"Data Validation Stage\"\\ntry:\\n    logging.info(f\">>>> stage {STAGE_NAME} started <<<<\")\\n    data_validation = DataValidationPipeline()\\n    data_validation.main()\\n    logging.info(f\">>>> stage {STAGE_NAME} completed <<<<\")\\nexcept Exception as e:\\n    logging.exception(e)\\n    raise e\\n\\n\\nSTAGE_NAME = \"Data Transformation Stage\"\\ntry:\\n    logging.info(f\">>>> stage {STAGE_NAME} started <<<<\")\\n    data_transformation = DataTransformationPipeline()\\n    data_transformation.main()\\n    logging.info(f\">>>> stage {STAGE_NAME} completed <<<<\")\\nexcept Exception as e:\\n    logging.exception(e)\\n    raise e\\n\\n\\nSTAGE_NAME = \"Model Training\"\\ntry:\\n    logging.info(f\">>>> stage {STAGE_NAME} started <<<<\")\\n    model_train = ModelTrainerPipeline()\\n    model_train.main()\\n    logging.info(f\">>>> stage {STAGE_NAME} completed <<<<\")\\nexcept Exception as e:\\n    logging.exception(e)\\n    raise e\\n\\n\\n# STAGE_NAME = \"Test data prediction\"\\n# try:\\n#     logging.info(f\">>>> stage {STAGE_NAME} started <<<<\")\\n#     pred = PredictionPipeline()\\n#     pred.get_transformed_data()\\n#     pred.predict_data()\\n#     logging.info(f\">>>> stage {STAGE_NAME} completed <<<<\")\\n# except Exception as e:\\n#     logging.exception(e)\\n#     raise e', metadata={'source': 'test_repo\\\\main.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import setuptools\\n\\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as f:\\n    long_description = f.read()\\n\\n__version__ = \"0.0.0\"\\n\\nREPO_NAME = \"House_Price-Prediction\"\\nAUTHOR_USER_NAME = \"srivanoo21\"\\nSRC_REPO = \"house_pricing\"\\nAUTHOR_EMAIL = \"srivanoo21@gmail.com\"\\n\\n\\nsetuptools.setup(\\n\\n    name=SRC_REPO,\\n    version=__version__,\\n    author=AUTHOR_USER_NAME,\\n    author_email=AUTHOR_EMAIL,\\n    description=\"A small python package for house price prediction\",\\n    long_description=long_description,\\n    long_description_content_type=\"text/markdown\",\\n    url=f\"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}\",\\n    project_urls = {\\n        \"Bug Tracker\" : f\"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}/issues\",\\n    },\\n    package_dir={\"\": \"src\"},\\n    packages=setuptools.find_packages(where=\"src\"),\\n\\n)\\n', metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nproject_name = \"house_pricing\"\\n\\n\\n# Below list of files need to be created\\n\\nlist_of_files = [\\n    \".github/workflows/.gitkeep\",\\n    f\"src/{project_name}/__init__.py\",\\n    f\"src/{project_name}/components/__init__.py\",\\n    f\"src/{project_name}/utils/__init__.py\",\\n    f\"src/{project_name}/utils/common.py\",\\n    f\"src/{project_name}/logging/__init__.py\",\\n    f\"src/{project_name}/config/__init__.py\",\\n    f\"src/{project_name}/config/configuration.py\",\\n    f\"src/{project_name}/pipeline/__init__.py\",\\n    f\"src/{project_name}/entity/__init__.py\",\\n    f\"src/{project_name}/constants/__init__.py\",\\n    \"config/config.yaml\",\\n    \"params.yaml\",\\n    \"app.py\",\\n    \"main.py\",\\n    \"Dockerfile\",\\n    \"requirements.txt\",\\n    \"setup.py\",\\n    \"research/trials.ipynb\",\\n    \"test.py\"\\n]\\n\\n\\n# Below is the logic for creation of files and folders\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n    if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory: {filedir} for the file {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) ==0):\\n        with open(filepath, \\'w\\') as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n    else:\\n        logging.info(f\"{filename} already exists\")', metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\test.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nfrom src.house_pricing.logger import logging\\n\\ndef error_message_detail(error,error_detail:sys):\\n    _,_,exc_tb=error_detail.exc_info()\\n    file_name=exc_tb.tb_frame.f_code.co_filename\\n    error_message=\"Error occured in python script name [{0}] line number [{1}] error message[{2}]\".format(\\n     file_name,exc_tb.tb_lineno,str(error))\\n\\n    return error_message\\n\\n    \\nclass CustomException(Exception):\\n    def __init__(self,error_message,error_detail:sys):\\n        super().__init__(error_message)\\n        self.error_message=error_message_detail(error_message,error_detail=error_detail)\\n    \\n    def __str__(self):\\n        return self.error_message\\n    \\n\\n\\n        ', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\exception.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import logging\\nimport os\\nfrom datetime import datetime\\n\\nLOG_FILE=f\"{datetime.now().strftime(\\'%m_%d_%Y_%H_%M_%S\\')}.log\"\\nlogs_path=os.path.join(os.getcwd(),\"logs\",LOG_FILE)\\nos.makedirs(logs_path, exist_ok=True)\\n\\nLOG_FILE_PATH=os.path.join(logs_path,LOG_FILE)\\n\\nlogging.basicConfig(\\n    filename=LOG_FILE_PATH,\\n    format=\"[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s\",\\n    level=logging.INFO,\\n)\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\logger.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nfrom src.house_pricing.exception import CustomException\\nfrom src.house_pricing.logger import logging\\nfrom src.house_pricing.entity import DataIngestionConfig\\nimport pandas as pd\\n\\n\\n\\nclass DataIngestion:\\n    def __init__(self):\\n        self.ingestion_config=DataIngestionConfig()\\n\\n    def initiate_data_ingestion(self):\\n        logging.info(\"Entered the data ingestion method or component\")\\n        try:\\n            train_set = pd.read_csv(\\'data\\\\house_train.csv\\')\\n            logging.info(\\'Read the training dataset as dataframe\\')\\n\\n            test_set = pd.read_csv(\\'data\\\\house_test.csv\\')\\n            logging.info(\\'Read the testing dataset as dataframe\\')\\n\\n            os.makedirs(os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True)\\n\\n            train_set.to_csv(self.ingestion_config.train_data_path, index=False, header=True)\\n            test_set.to_csv(self.ingestion_config.test_data_path, index=False, header=True)\\n\\n            logging.info(\"Train and test data has been saved\")\\n            logging.info(\"Ingestion of the data is completed\")\\n\\n            return(\\n                self.ingestion_config.train_data_path,\\n                self.ingestion_config.test_data_path\\n            )\\n        except Exception as e:\\n            raise CustomException(e, sys)\\n        \\nif __name__==\"__main__\":\\n    obj = DataIngestion()\\n    train_data, test_data = obj.initiate_data_ingestion()\\n\\n    #data_transformation=DataTransformation()\\n    #train_arr,test_arr,_=data_transformation.initiate_data_transformation(train_data,test_data)\\n\\n    #modeltrainer=ModelTrainer()\\n    #print(modeltrainer.initiate_model_trainer(train_arr,test_arr))\\n\\n\\n\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nimport os\\nimport numpy as np \\nimport pandas as pd\\nfrom scipy.stats import skew\\nfrom dataclasses import dataclass\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\\nfrom src.house_pricing.exception import CustomException\\nfrom src.house_pricing.logger import logging\\nfrom src.house_pricing.entity import DataTransformationConfig\\nfrom src.house_pricing.utils import save_object\\n\\n\\nclass DataTransformation:\\n    def __init__(self):\\n        self.data_transformation_config = DataTransformationConfig()\\n\\n\\n    def cat_con_df(self, test_data):\\n        \\'\\'\\'\\n        Separating the categorical and continuous columns from the testing dataset\\'\\'\\'\\n        \\n        cat = []\\n        con = []\\n\\n        for i in test_data.columns:\\n            if (test_data[i].dtypes==\\'object\\'):\\n                cat.append(i)\\n            else:\\n                con.append(i)\\n        con.remove(\\'Id\\')\\n        \\n        return cat, con\\n\\n\\n    def fill_top_missing_values(self, test_data):\\n        \\'\\'\\'\\n        Check and fill the top 5 missing columns with \\'0\\' in the testing dataset\\'\\'\\'\\n\\n        miss1 = (test_data.isna().sum()/test_data.shape[0])*100\\n        miss1 = pd.DataFrame(miss1, columns=[\\'count\\'])\\n        miss1 = miss1.sort_values(by=\\'count\\', ascending=False)\\n\\n        logging.info(f\"Top 10 missing features from testing dataset are: {miss1[:10]}\")\\n\\n        test_miss = (miss1[:6].index).values\\n\\n        for i in test_miss:\\n            test_data[i].fillna(\"0\", inplace=True)\\n        \\n        logging.info(f\"Missing features from testing dataset are now replaced with \\'0\\'\")\\n            \\n        return test_data\\n\\n\\n    def fill_missing_values(self, test_data):\\n        \\'\\'\\'\\n        Removing the rest of the missing columns from the testing dataset\\'\\'\\'\\n\\n        cat, con = self.cat_con_df(test_data)\\n        test_data = self.fill_top_missing_values(test_data)\\n\\n        si1 = SimpleImputer(strategy=\\'mean\\')\\n        si2 = SimpleImputer(strategy=\\'most_frequent\\')\\n\\n        A = pd.DataFrame(si1.fit_transform(test_data[con]), columns=con)\\n        B = pd.DataFrame(si2.fit_transform(test_data[cat]), columns=cat)\\n\\n        test_new = A.join(B)\\n        logging.info(f\"Missing features from testing dataset are now handled\")\\n        \\n        return test_new, cat, con\\n\\n    \\n    def check_skew(self, test_data):\\n        \\'\\'\\'\\n        Check the skewness of the testing data\\'\\'\\'\\n        \\n        test_data, cat, con = self.fill_missing_values(test_data)\\n\\n        skewed = test_data[con].apply(lambda x: skew(x))\\n        skewed = skewed[skewed > 0.75]\\n        skewed = skewed.index\\n\\n        test_data[skewed] = np.log1p(test_data[skewed])\\n        logging.info(f\"skewness has now been removed from the testing data\")\\n        \\n        return test_data, cat, con\\n    \\n\\n    def scaling(self, test_data):\\n        \\'\\'\\'\\n        perform scaling of the test data\\'\\'\\'\\n        \\n        ss = StandardScaler()\\n        test_data, cat, con = self.check_skew(test_data)\\n        ## SS object need to be in artifacts\\n        test_data[con] = ss.fit_transform(test_data[con])\\n        logging.info(f\"Feature scaling is now performed\")\\n        \\n        return test_data, cat, con\\n    \\n\\n    def handle_outliers(self, test_data):\\n        \\'\\'\\'\\n        Removing the outliers from the below columns as they have more number of outliers\\'\\'\\'\\n        test_data, cat, con = self.scaling(test_data)\\n        val1 = [\\'BsmtUnfSF\\', \\'TotalBsmtSF\\', \\'KitchenAbvGr\\', \\'ScreenPorch\\']\\n\\n        for i in val1:\\n            Q1 = test_data[i].quantile(0.05)\\n            Q3 = test_data[i].quantile(0.95)\\n            IQR = Q3 - Q1\\n            test_data = test_data[(test_data[i] >= Q1 - 1.5*IQR) & (test_data[i] <= Q3 + 1.5*IQR)]\\n\\n        logging.info(f\"Outliers are now handled in the testing data\")\\n            \\n        return test_data, cat, con\\n    \\n\\n    def encode_test_data(self, test_data):\\n        \\'\\'\\'\\n        perform one hot encoding so as to handle unseen values\\'\\'\\'\\n        \\n        test_data, cat, con = self.handle_outliers(test_data)\\n        le = LabelEncoder()\\n        for i in cat:\\n            test_data[i] = le.fit_transform(test_data[i])\\n            \\n        logging.info(f\"Missing features from testing dataset are now replaced with \\'0\\'\")\\n        return test_data\\n    \\n\\n    def initiate_data_transformation(self):\\n        \\'\\'\\'\\n        Start the data transformation process\\'\\'\\'\\n\\n        try:\\n            test_data = pd.read_csv(self.data_transformation_config.test_data_path)\\n            logging.info(\"Read testing data is completed\")\\n\\n            test_data = self.encode_test_data(test_data)\\n            logging.info(f\"Data pre-processing is completed for testing data\")\\n            \\n            os.makedirs(os.path.dirname(self.data_transformation_config.preprocessed_test_data_path), \\n                        exist_ok=True)\\n            logging.info(\"The directory for the data transformation is now created\")\\n\\n            logging.info(f\"Saving preprocessed testing data.\")\\n\\n            save_object(\\n                file_path = self.data_transformation_config.preprocessed_test_data_path,\\n                obj = test_data\\n            )\\n            logging.info(f\"Saved preprocessed testing data.\")\\n\\n            return self.data_transformation_config.preprocessed_test_data_path\\n            \\n        except Exception as e:\\n            raise CustomException(e,sys)\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\components\\\\data_transformation_prediction.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nimport os\\nimport numpy as np \\nimport pandas as pd\\nfrom scipy.stats import skew\\nfrom dataclasses import dataclass\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\\nfrom src.house_pricing.exception import CustomException\\nfrom src.house_pricing.logger import logging\\nfrom src.house_pricing.entity import DataTransformationConfig\\n\\n\\n\\nclass DataTransformation:\\n    def __init__(self):\\n        self.data_transformation_config = DataTransformationConfig()\\n\\n\\n    def cat_con_df(self, train_data):\\n        \\'\\'\\'\\n        Separating the categorical and continuous columns from the training dataset\\'\\'\\'\\n        \\n        cat = []\\n        con = []\\n\\n        for i in train_data.columns:\\n            if (train_data[i].dtypes==\\'object\\'):\\n                cat.append(i)\\n            else:\\n                con.append(i)\\n        con.remove(\\'Id\\')\\n        \\n        return cat, con\\n\\n\\n    def fill_top_missing_values(self, train_data):\\n        \\'\\'\\'\\n        Check and fill the top 5 missing columns with \\'0\\' in the training dataset\\'\\'\\'\\n\\n        miss1 = (train_data.isna().sum()/train_data.shape[0])*100\\n        miss1 = pd.DataFrame(miss1, columns=[\\'count\\'])\\n        miss1 = miss1.sort_values(by=\\'count\\', ascending=False)\\n\\n        logging.info(f\"Top 10 missing features from training dataset are: {miss1[:10]}\")\\n\\n        train_miss = (miss1[:6].index).values\\n\\n        for i in train_miss:\\n            train_data[i].fillna(\"0\", inplace=True)\\n        \\n        logging.info(f\"Missing features from training dataset are now replaced with \\'0\\'\")\\n            \\n        return train_data\\n\\n\\n    def fill_missing_values(self, train_data):\\n        \\'\\'\\'\\n        Removing the rest of the missing columns from the training dataset\\'\\'\\'\\n\\n        cat, con = self.cat_con_df(train_data)\\n        train_data = self.fill_top_missing_values(train_data)\\n\\n        si1 = SimpleImputer(strategy=\\'mean\\')\\n        si2 = SimpleImputer(strategy=\\'most_frequent\\')\\n\\n        A = pd.DataFrame(si1.fit_transform(train_data[con]), columns=con)\\n        B = pd.DataFrame(si2.fit_transform(train_data[cat]), columns=cat)\\n\\n        train_new = A.join(B)\\n        logging.info(f\"Missing features from training dataset are now handled\")\\n        \\n        return train_new, cat, con\\n\\n    \\n    def check_skew(self, train_data):\\n        \\'\\'\\'\\n        Check the skewness of the training data\\'\\'\\'\\n        \\n        train_data, cat, con = self.fill_missing_values(train_data)\\n\\n        con.remove(\\'SalePrice\\')\\n        skewed = train_data[con].apply(lambda x: skew(x))\\n        skewed = skewed[skewed > 0.75]\\n        skewed = skewed.index\\n\\n        train_data[skewed] = np.log1p(train_data[skewed])\\n        con.append(\\'SalePrice\\')\\n        logging.info(f\"skewness has now been removed from the training data\")\\n        \\n        return train_data, cat, con\\n\\n\\n    def scaling(self, train_data):\\n        \\'\\'\\'\\n        perform scaling of the train data\\'\\'\\'\\n        \\n        ss = StandardScaler()\\n        train_data, cat, con = self.check_skew(train_data)\\n\\n        con.remove(\\'SalePrice\\')\\n        train_data[con] = ss.fit_transform(train_data[con])\\n        con.append(\\'SalePrice\\')\\n        logging.info(f\"Feature scaling is now performed\")\\n        \\n        return train_data, cat, con\\n    \\n\\n    def handle_outliers(self, train_data):\\n        \\'\\'\\'\\n        Removing the outliers from the below columns as they have more number of outliers\\'\\'\\'\\n        train_data, cat, con = self.scaling(train_data)\\n        val1 = [\\'BsmtUnfSF\\', \\'TotalBsmtSF\\', \\'KitchenAbvGr\\', \\'ScreenPorch\\']\\n\\n        for i in val1:\\n            Q1 = train_data[i].quantile(0.05)\\n            Q3 = train_data[i].quantile(0.95)\\n            IQR = Q3 - Q1\\n            train_data = train_data[(train_data[i] >= Q1 - 1.5*IQR) & (train_data[i] <= Q3 + 1.5*IQR)]\\n\\n        logging.info(f\"Outliers are now handled in the training data\")\\n            \\n        return train_data, cat, con\\n    \\n\\n    def encode_train_data(self, train_data):\\n        \\'\\'\\'\\n        perform one hot encoding so as to handle unseen values\\'\\'\\'\\n        train_data, cat, con = self.handle_outliers(train_data)\\n        le = LabelEncoder()\\n        for i in cat:\\n            train_data[i] = le.fit_transform(train_data[i])\\n            \\n        logging.info(f\"Missing features from training dataset are now replaced with \\'0\\'\")\\n        return train_data\\n    \\n\\n    def initiate_data_transformation(self):\\n        \\'\\'\\'\\n        Start the data transformation process\\'\\'\\'\\n\\n        try:\\n            train_data = pd.read_csv(self.data_transformation_config.train_data_path)\\n            logging.info(\"Read training data is completed\")\\n\\n            train_data = self.encode_train_data(train_data)\\n            logging.info(f\"Data pre-processing is completed for training data\")\\n            \\n            os.makedirs(os.path.dirname(self.data_transformation_config.preprocessed_train_data_path), \\n                        exist_ok=True)\\n            logging.info(\"The directory for the data transformation is now created\")\\n\\n            logging.info(f\"Saving preprocessed training data.\")\\n            train_data.to_csv(self.data_transformation_config.preprocessed_train_data_path, \\n                              index=False, header=True)\\n            \\n            logging.info(f\"Saved preprocessed training data.\")\\n\\n            return self.data_transformation_config.preprocessed_train_data_path\\n            \\n        except Exception as e:\\n            raise CustomException(e,sys)\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\components\\\\data_transformation_train.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom src.house_pricing.logger import logging\\nfrom src.house_pricing.entity import DataValidationConfig\\n\\n\\nclass DataValidation:\\n    \\n    def __init__(self, config: DataValidationConfig):\\n        self.config = config\\n\\n    def validate_all_files_exist(self) -> bool:\\n        try:\\n            validation_status = None\\n            all_files = os.listdir(os.path.join(\"artifacts\", \"data_ingestion\"))\\n            self.config.ALL_REQUIRED_FILES = [\"house_training_data.csv\", \"house_testing_data.csv\"]\\n\\n            # create the status file\\n            os.makedirs(self.config.root_dir, exist_ok=True)\\n            logging.info(\"Directory created for data validation\")\\n\\n            status_file = os.path.join(self.config.root_dir, self.config.STATUS_FILE)\\n\\n            for file in all_files:\\n                if (file not in self.config.ALL_REQUIRED_FILES):\\n                    validation_status = False\\n                    with open(status_file, \"w\") as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n                    return validation_status\\n                else:\\n                    validation_status = True\\n                    with open(status_file, \"w\") as f:\\n                        f.write(f\"Validation status: {validation_status}\")\\n\\n            logging.info(\"Validation status has been updated\")\\n            \\n        except Exception as e:\\n            raise e\\n        \\n        return validation_status\\n        ', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\components\\\\data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nimport os\\nimport numpy as np \\nimport pandas as pd\\nfrom scipy.stats import skew\\nfrom dataclasses import dataclass\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\\nfrom src.house_pricing.exception import CustomException\\nfrom src.house_pricing.logger import logging\\nfrom src.house_pricing.entity import ModelTrainerConfig\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import r2_score\\nfrom sklearn.tree import DecisionTreeRegressor\\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\\nfrom xgboost import XGBRegressor\\nfrom src.house_pricing.utils import save_object\\n\\n\\nclass ModelTrainer:\\n    def __init__(self):\\n        self.model_trainer_config = ModelTrainerConfig()\\n\\n\\n    def train_val_split(self, train_data, target_column):\\n        \\'\\'\\'\\n        split the input data into training and validation set\\'\\'\\'\\n\\n        X = train_data.drop(labels=[target_column], axis=1)\\n        Y = train_data[target_column]\\n        logging.info(\"Data spitting for input and target columns are now prepared\")\\n\\n        xtrain, xval, ytrain, yval = train_test_split(X, Y, test_size=0.2, random_state=40)\\n        logging.info(f\"The shape of training data is {xtrain.shape}\")\\n        logging.info(f\"The shape of validation data is {xval.shape}\")\\n        \\n        return xtrain, xval, ytrain, yval\\n\\n\\n\\n    def initiate_model_trainer(self):\\n        \\'\\'\\'\\n        initiate the model training for the training set\\'\\'\\'\\n\\n        try:\\n            train_data_path = self.model_trainer_config.preprocessed_train_data_path\\n            train_data = pd.read_csv(train_data_path)\\n            target_column = self.model_trainer_config.target_column\\n            logging.info(\"Pre-processed training dataset and target column is now loaded\")\\n\\n            xtrain, xval, ytrain, yval = self.train_val_split(train_data, target_column)\\n            logging.info(\"Splitting of training and validation data is now completed\")\\n\\n            models = {\\n                \"Linear Regression\": LinearRegression(),\\n                \"Decision Tree\": DecisionTreeRegressor(),\\n                \"Random Forest\": RandomForestRegressor(),\\n                \"AdaBoost Regressor\": AdaBoostRegressor()\\n                # \"Gradient Boosting\": GradientBoostingRegressor(),\\n                # \"XGBRegressor\": XGBRegressor()\\n            }\\n\\n            params={\\n                \"Linear Regression\":{},\\n                \"Decision Tree\": {\\n                    \\'criterion\\':[\\'squared_error\\', \\'friedman_mse\\', \\'absolute_error\\', \\'poisson\\'],\\n                    # \\'splitter\\':[\\'best\\',\\'random\\'],\\n                    # \\'max_features\\':[\\'sqrt\\',\\'log2\\'],\\n                },\\n                \"Random Forest\":{\\n                    # \\'criterion\\':[\\'squared_error\\', \\'friedman_mse\\', \\'absolute_error\\', \\'poisson\\'],\\n                 \\n                    # \\'max_features\\':[\\'sqrt\\',\\'log2\\',None],\\n                    \\'n_estimators\\': [8,16,32,64,128,256]\\n                },\\n                # \"Gradient Boosting\":{\\n                #     # \\'loss\\':[\\'squared_error\\', \\'huber\\', \\'absolute_error\\', \\'quantile\\'],\\n                #     \\'learning_rate\\':[.1,.01,.05,.001],\\n                #     \\'subsample\\':[0.6,0.7,0.75,0.8,0.85,0.9],\\n                #     # \\'criterion\\':[\\'squared_error\\', \\'friedman_mse\\'],\\n                #     # \\'max_features\\':[\\'auto\\',\\'sqrt\\',\\'log2\\'],\\n                #     \\'n_estimators\\': [8,16,32,64,128,256]\\n                # },\\n                \\n                # \"XGBRegressor\":{\\n                #     \\'learning_rate\\':[.1,.01,.05,.001],\\n                #     \\'n_estimators\\': [8,16,32,64,128,256]\\n                # },\\n                # \"CatBoosting Regressor\":{\\n                #     \\'depth\\': [6,8,10],\\n                #     \\'learning_rate\\': [0.01, 0.05, 0.1],\\n                #     \\'iterations\\': [30, 50, 100]\\n                # },\\n                \"AdaBoost Regressor\":{\\n                    \\'learning_rate\\':[.1,.01,0.5,.001],\\n                    # \\'loss\\':[\\'linear\\',\\'square\\',\\'exponential\\'],\\n                    \\'n_estimators\\': [8,16,32,64,128,256]\\n                }   \\n            }\\n\\n            train_report, val_report, cross_val_report, adj_r2_report = self.evaluate_models(\\n                train_data, target_column, xtrain, ytrain, xval, yval, models, params)\\n            \\n            logging.info(f\"The training report is {train_report}\")\\n            logging.info(f\"The validation report is {val_report}\")\\n            logging.info(f\"The cross validation report is {cross_val_report}\")\\n            logging.info(f\"The adj_r2_report report is {adj_r2_report}\")\\n            \\n            ## To get best model score from dict\\n            best_train_score = max(sorted(train_report.values()))\\n            best_val_score = max(sorted(val_report.values()))\\n            best_cross_val_score = max(sorted(cross_val_report.values()))\\n            best_adj_r2_score = max(sorted(adj_r2_report.values()))\\n\\n            ## To get best model name from dict on different areas\\n            best_model_name_train = list(train_report.keys())[\\n                list(train_report.values()).index(best_train_score)]\\n            best_model_train = models[best_model_name_train]\\n\\n            best_model_name_val = list(val_report.keys())[\\n                list(val_report.values()).index(best_val_score)]\\n            best_model_val = models[best_model_name_val]\\n\\n            best_model_name_cross_val = list(cross_val_report.keys())[\\n                list(cross_val_report.values()).index(best_cross_val_score)]\\n            best_model_cross_val = models[best_model_name_cross_val]\\n\\n            best_model_name_adj_r2 = list(adj_r2_report.keys())[\\n                list(adj_r2_report.values()).index(best_adj_r2_score)]\\n            best_model_adj_r2 = models[best_model_name_adj_r2]\\n\\n            # Displaying which model provides best score in which area\\n            logging.info(f\"Best model for training score is: {best_model_train}\")\\n            logging.info(f\"Best model for validation score is: {best_model_val}\")\\n            logging.info(f\"Best model for cross validation score is: {best_model_cross_val}\")\\n            logging.info(f\"Best model for adjusted r2 score is: {best_model_adj_r2}\")\\n\\n            # Compare the best model for adjusted r2 score is for the threshold\\n            if best_adj_r2_score < 0.6:\\n                raise CustomException(\"No best model found\")\\n            logging.info(f\"Best found model on both training and validation dataset\")\\n\\n            save_object(\\n                file_path = self.model_trainer_config.trained_model_file_path,\\n                obj = best_model_adj_r2\\n            )\\n            logging.info(f\"Best Model is now saved in the artifacts in {self.model_trainer_config.trained_model_file_path}\")\\n                        \\n        except Exception as e:\\n            raise CustomException(e, sys)\\n        \\n\\n\\n    def evaluate_models(self, train_data, target_column, xtrain, ytrain, xval, yval, models, param):\\n        \\'\\'\\'\\n        evaluate all the models using various metrics\\'\\'\\'\\n\\n        try:\\n            report_train_score = {}\\n            report_val_score = {}\\n            report_cross_val_score = {}\\n            report_adjusted_r2_score = {}\\n\\n            for i in range(len(list(models))):\\n                model = list(models.values())[i]\\n                para = param[list(models.keys())[i]]\\n\\n                gs = GridSearchCV(model, para, cv=3)\\n                gs.fit(xtrain, ytrain)\\n\\n                model.set_params(**gs.best_params_)\\n                model.fit(xtrain, ytrain)\\n                mean_cross_val_score = np.abs(np.mean(cross_val_score(model, xtrain, ytrain, scoring=\\'neg_mean_absolute_error\\', cv=3)))\\n\\n                ytrain_pred = model.predict(xtrain)\\n                yval_pred = model.predict(xval)\\n                \\n                train_model_score = r2_score(ytrain, ytrain_pred)\\n                val_model_score = r2_score(yval, yval_pred)\\n\\n                n = xval.shape[0]\\n                p = xval.shape[1]\\n                adjusted_val_score = 1-(1-val_model_score)*(n-1)/(n-p-1)\\n\\n                report_train_score[list(models.keys())[i]] = train_model_score\\n                report_val_score[list(models.keys())[i]] = val_model_score\\n                report_cross_val_score[list(models.keys())[i]] = mean_cross_val_score\\n                report_adjusted_r2_score[list(models.keys())[i]] = adjusted_val_score\\n\\n            return report_train_score, report_val_score, report_cross_val_score, report_adjusted_r2_score\\n\\n        except Exception as e:\\n            raise CustomException(e, sys)\\n        ', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\components\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nfrom dataclasses import dataclass\\nfrom src.house_pricing.constants import *\\n\\n\\nclass ConfigurationManager:\\n    \\n    def __init__(self, config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH):\\n        \\n        #self.config = read_yaml(config_filepath)\\n        #self.params = read_yaml(params_filepath)\\n\\n        #create_directories([self.config.artifacts_root])\\n        pass', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\config\\\\configuration.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\config\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pathlib import Path\\n\\nCONFIG_FILE_PATH = Path(\"config/config.yaml\")\\nPARAMS_FILE_PATH = Path(\"params.yaml\")\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\constants\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from dataclasses import dataclass\\nimport os\\n\\n@dataclass\\nclass DataIngestionConfig:\\n    train_data_path: str = os.path.join(\\'artifacts\\', \"data_ingestion\", \"house_training_data.csv\")\\n    test_data_path: str = os.path.join(\\'artifacts\\', \"data_ingestion\", \"house_testing_data.csv\")\\n\\n\\n@dataclass\\nclass DataValidationConfig:\\n    ALL_REQUIRED_FILES: list\\n    root_dir: str = os.path.join(\"artifacts\", \"data_validation\")\\n    STATUS_FILE: str = \"status.txt\"\\n\\n\\n@dataclass\\nclass DataTransformationConfig:\\n    train_data_path = os.path.join(\\'artifacts\\', \"data_ingestion\", \"house_training_data.csv\")\\n    test_data_path = os.path.join(\\'artifacts\\', \"data_ingestion\", \"house_testing_data.csv\")\\n    preprocessed_train_data_path = os.path.join(\\'artifacts\\', \"data_transformation\", \"preprocesed_train_data.csv\")\\n    preprocessed_test_data_path = os.path.join(\\'artifacts\\', \"data_transformation\", \"preprocesed_test_data.csv\")\\n\\n\\n@dataclass\\nclass ModelTrainerConfig:\\n    target_column: str = \"SalePrice\"\\n    preprocessed_train_data_path = os.path.join(\\'artifacts\\', \"data_transformation\", \"preprocesed_train_data.csv\")\\n    trained_model_file_path: str = os.path.join(\"artifacts\", \"model_train\", \"model.pkl\")\\n\\n\\n@dataclass\\nclass PredictionConfig:\\n    predicted_column: str = \"SalePrice\"\\n    preprocessed_test_data_path = os.path.join(\\'artifacts\\', \"data_transformation\", \"preprocesed_test_data.csv\")\\n    trained_model_file_path: str = os.path.join(\"artifacts\", \"model_train\", \"model.pkl\")\\n    predicted_data_path = os.path.join(\"artifacts\", \"data_prediction\", \"predicted_data.csv\")', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\entity\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='#from src.house_pricing.config.configuration import ConfigurationManager\\nfrom src.house_pricing.components.model_trainer import ModelTrainer\\nfrom src.house_pricing.entity import PredictionConfig\\nfrom src.house_pricing.components.data_transformation_prediction import DataTransformation\\nfrom src.house_pricing.logger import logging\\nfrom src.house_pricing.utils import save_object, load_object\\n\\n\\nclass PredictionPipeline:\\n    def __init__(self):\\n        self.config = PredictionConfig()\\n\\n\\n    def get_transformed_data(self):\\n        \\'\\'\\'\\n        get the transformed data for the prediction\\'\\'\\'\\n\\n        try:\\n            #config = ConfigurationManager()\\n            #data_validation_config = config.get_data_validation_config()\\n            data_transformation = DataTransformation()\\n            data_transformation.initiate_data_transformation()\\n\\n        except Exception as e:\\n            raise e\\n        \\n\\n    def predict_data(self):\\n        \\'\\'\\'\\n        perform prediction on test data and return the output along with the predicted values\\'\\'\\'\\n\\n        try:\\n            test_data_path = self.config.preprocessed_test_data_path\\n            test_data = load_object(file_path = test_data_path)\\n            logging.info(\"Test data to be predicted is fetched\")\\n                         \\n            model_path = self.config.trained_model_file_path\\n            model = load_object(file_path = model_path)\\n            logging.info(\"Model is loaded from the artifacts\")\\n\\n            predicted_column = self.config.predicted_column\\n            \\n            predicted_data = model.predict(test_data)\\n            logging.info(\"Prediction is performed on the test data\")\\n\\n            test_data[predicted_column] = predicted_data\\n            logging.info(\"Predicted data is now added in the test dataset\")\\n\\n            save_object(\\n                file_path = self.config.predicted_data_path,\\n                obj = test_data\\n            )\\n            logging.info(\"Test dataset after the prediction is now saved successfully in the artifacts\")\\n\\n        except Exception as e:\\n            raise e\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from src.house_pricing.config.configuration import ConfigurationManager\\nfrom src.house_pricing.components.data_ingestion import DataIngestion\\nfrom src.house_pricing.logger import logging\\n\\n\\nclass DataIngestionPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        try:\\n            #config = ConfigurationManager()\\n            #data_ingestion_config = config.get_data_ingestion_config()\\n            data_ingestion = DataIngestion()\\n            train_data, test_data = data_ingestion.initiate_data_ingestion()\\n\\n        except Exception as e:\\n            raise e', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\pipeline\\\\stage_01_data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='#from src.house_pricing.config.configuration import ConfigurationManager\\nfrom src.house_pricing.components.data_validation import DataValidation\\nfrom src.house_pricing.entity import DataValidationConfig\\nfrom src.house_pricing.logger import logging\\n\\n\\nclass DataValidationPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        try:\\n            pass\\n            #config = ConfigurationManager()\\n            #data_validation_config = config.get_data_validation_config()\\n            data_validation = DataValidation(config=DataValidationConfig)\\n            data_validation.validate_all_files_exist()\\n\\n        except Exception as e:\\n            raise e\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\pipeline\\\\stage_02_data_validation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='#from src.house_pricing.config.configuration import ConfigurationManager\\nfrom src.house_pricing.components.data_transformation_train import DataTransformation\\nfrom src.house_pricing.entity import DataTransformationConfig\\nfrom src.house_pricing.logger import logging\\n\\n\\nclass DataTransformationPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        try:\\n            pass\\n            #config = ConfigurationManager()\\n            #data_validation_config = config.get_data_validation_config()\\n            data_transformation = DataTransformation()\\n            data_transformation.initiate_data_transformation()\\n\\n        except Exception as e:\\n            raise e\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\pipeline\\\\stage_03_data_transformation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='#from src.house_pricing.config.configuration import ConfigurationManager\\nfrom src.house_pricing.components.model_trainer import ModelTrainer\\nfrom src.house_pricing.entity import ModelTrainerConfig\\nfrom src.house_pricing.logger import logging\\n\\n\\nclass ModelTrainerPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def main(self):\\n        try:\\n            pass\\n            #config = ConfigurationManager()\\n            #data_validation_config = config.get_data_validation_config()\\n            model_train = ModelTrainer()\\n            model_train.initiate_model_trainer()\\n\\n        except Exception as e:\\n            raise e\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\pipeline\\\\stage_04_model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\pipeline\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\utils\\\\common.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nimport numpy as np \\nimport pandas as pd\\nimport pickle\\nfrom sklearn.metrics import r2_score\\nfrom sklearn.model_selection import GridSearchCV\\nfrom src.house_pricing.exception import CustomException\\n\\n    \\ndef save_object(file_path, obj):\\n    try:\\n        dir_path = os.path.dirname(file_path)\\n\\n        os.makedirs(dir_path, exist_ok=True)\\n\\n        with open(file_path, \"wb\") as file_obj:\\n            pickle.dump(obj, file_obj, pickle.DEFAULT_PROTOCOL)\\n\\n    except Exception as e:\\n        raise CustomException(e, sys)\\n\\n    \\n    \\ndef load_object(file_path):\\n    try:\\n        with open(file_path, \"rb\") as file_obj:\\n            return pickle.load(file_obj)\\n\\n    except Exception as e:\\n        raise CustomException(e, sys)\\n', metadata={'source': 'test_repo\\\\src\\\\house_pricing\\\\utils\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 2000,\n",
    "                                                             chunk_overlap = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers<5.0.0,>=4.32.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "Requirement already satisfied: tqdm in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: numpy in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from sentence-transformers) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
      "Requirement already satisfied: sympy in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\data_science_stuff\\generative_ai_classes\\code_analyzer\\venv\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
      "Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "Installing collected packages: transformers, sentence-transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'd:\\\\Data_Science_stuff\\\\Generative_AI_classes\\\\code_analyzer\\\\venv\\\\Lib\\\\site-packages\\\\transformers\\\\models\\\\bark\\\\configuration_bark.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lenovo\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "#embeddings = OpenAIEmbeddings()\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge base (vector DB)\n",
    "\n",
    "Here we are using the Chroma library to create a vector database (vectordb) from a list of text documents (texts) with associated embeddings (embeddings).\n",
    "\n",
    "The 'persist_directory' is a directory where the vector database will be stored as arguments.\n",
    "After creating the vector database, 'persist()' is called to save the vector database to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding_function, persist_directory='./db')\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In essence, this line sets up a memory component specifically designed to store and potentially retrieve a \n",
    "# conversation history associated with a particular LLM. This history can then be used by the LLM to provide \n",
    "# more contextually relevant responses in an ongoing conversation.\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This suggests the retrieval method might use a technique called \"Maximum Marginal Relevance\" (MMR). \n",
    "# MMR aims to retrieve documents that are both relevant to the query and dissimilar to each other, \n",
    "# promoting diversity in the retrieved answers.\n",
    "\n",
    "# This line configures a question-answering component within a conversational system. \n",
    "# It creates a retrieval chain specifically designed for the provided LLM, defines a retrieval method that \n",
    "# leverages an MMR-based search, and potentially incorporates the conversation history for contextually \n",
    "# relevant answers.\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3}), memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is DataIngestion class?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data_Science_stuff\\Generative_AI_classes\\code_analyzer\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the `DataIngestion` class is to provide functionality for ingesting and managing data through pipelines. The class includes methods for handling data ingestion, validation, transformation, and model training tasks, which are essential stages in a typical data science pipeline.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
